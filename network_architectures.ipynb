{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architectures to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liu et al.\n",
    "did random search\n",
    "\n",
    "- MLP with 4 layers at 400 neurons each\n",
    "- ReLU activation (paper didn't try LeakyReLU\n",
    "- no dropout\n",
    "- no batch norm\n",
    "- glorot uniform initialization\n",
    "- Adam optimizer\n",
    "- 1024 batch size, 200 epochs\n",
    "\n",
    "**first try regression on equilibrium** using this architecture and compare with B-S accuracy, then **try multi-task learning** with two output nodes that each do a regression in bid and ask to forecast the market to make\n",
    "\n",
    "batch normalization and dropout do not help with accuracy because the features are actually informative and not redundant unlike the unstructured data in an image (say we found this in line with Liu et al.)\n",
    "\n",
    "they did provide the implied volatilities, or they provided the option prices to compute the implied volatilities\n",
    "\n",
    "learning rate optimal $10^{-5} - 10^{-3}$\n",
    "\n",
    "- we sampled learning rate 1e-3 and didn't converge, 1e-4 better, 1e-5 also good,  but 1e-6 start to be bad\n",
    "\n",
    "in Liu et al. it is shown this network architecture can learn the Black-Scholes and Heston models\n",
    "\n",
    "they look at $R^2$, RMSE, MAE, MAPE\n",
    "\n",
    "similar MLP architecture with less neurons: https://towardsdatascience.com/neural-networks-for-option-pricing-danielcotto-c24569ad0bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlackScholesNN.pdf (github paper)\n",
    "\n",
    "- 4 hidden layers at 100 neurons each\n",
    "- 25% dropout\n",
    "- output layer used exponential, while hidden used LeakyReLU, ELU, ReLU, ELU\n",
    "- kinda stupid, so don't really use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can explain volatility smile, maybe see if it can learn the volatility smile by not cleaning away that data\n",
    "\n",
    "using NN to value the difference between B-S and the actual option price in \"A Hybrid Option Pricing Model Using a Neural Network for Estimating Volatility\" also \"Application of Machine Learning: An Analysis of Asian Options Pricing Using Neural Network\"\n",
    "\n",
    "https://www.tandfonline.com/doi/figure/10.1080/03081070701210303?scroll=top&needAccess=true\n",
    "\n",
    "https://ieeexplore.ieee.org/document/8119142\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## look into RNN architectures\n",
    "\n",
    "- do RNN with 21 day (one trading month) moving window, or 5 day or 15 day or whatever kind of search\n",
    "\n",
    "https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2260069\n",
    "\n",
    "the LSTM may only need to be run on the stock price itself, otherwise we may have problems in finding particular options that have been trading for 21 days to use the LSTM on\n",
    "\n",
    "```\n",
    "# https://blog.usejournal.com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oslo paper\n",
    "http://jultika.oulu.fi/files/nbnfioulu-201901091016.pdf\n",
    "\n",
    "- used CNN similar to AlexNet\n",
    "- used 30, 60, 90, 120 day volatility estimates all fed into the network\n",
    "- Adam with $10^{-3}$ learning rate and default betas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pdfs.semanticscholar.org/c2c6/a6d2a1b91bdde95cf2f24ada7fe0750c2062.pdf\n",
    "\n",
    "- uses moneyness of option, and has filtering criteria, we choose not to because we have exponentially more data\n",
    "- try with an without homogeneity assumption\n",
    "- did not put volatilities into model\n",
    "- 100, 80, 60, 40 with all sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
